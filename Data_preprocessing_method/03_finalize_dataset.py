import json
import os
import sys
from transformers import AutoTokenizer

# =============================================================================
# [Configuration] 
# =============================================================================
CONFIG = {
    # 1. ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ (ì…ë ¥ -> ì¶œë ¥)
    "FILES_TO_PROCESS": [
        {
            "input": "/home/aikusrv01/storyboard/PSY/final_translated_dataset.jsonl",
            "output": "/home/aikusrv01/storyboard/PSY/train_dataset_final.jsonl"
        }
    ],

    # 2. í† í° í•„í„°ë§ ì„¤ì •
    "TOKEN_LIMIT": 77,     # CLIP ëª¨ë¸ ê¸°ì¤€ (ì¼ë°˜ì ìœ¼ë¡œ 75~77)
    "TOKENIZER_MODEL": "openai/clip-vit-large-patch14", # í† í¬ë‚˜ì´ì € ë¡œë“œìš© ëª¨ë¸ëª…

    # 3. íƒœê·¸ ì¬ì •ë ¬ ë° íŠ¸ë¦¬ê±° ì„¤ì •
    "TARGET_SHOTS": ['medium shot', 'close-up shot', 'full shot'],
    "TARGET_ANGLES": ['High angle', 'Low angle', 'Eye level'],
    "TRIGGER_MAP": {
        'medium shot': '<ms_trg>',
        'close-up shot': '<cu_trg>',
        'full shot': '<fs_trg>'
    }
}
# =============================================================================

def reorder_and_add_trigger(text):
    """
    [ê¸°ëŠ¥ 1] í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    - ìƒ·/ì•µê¸€ íƒœê·¸ë¥¼ ì°¾ì•„ ìˆœì„œë¥¼ [íŠ¸ë¦¬ê±°, ìƒ·, ì•µê¸€, ...]ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    - íŠ¸ë¦¬ê±° ì›Œë“œë¥¼ ë§¨ ì•ì— ë¶€ì°©í•©ë‹ˆë‹¤.
    """
    tags = [t.strip() for t in text.split(',')]
    
    found_shot = None
    found_angle = None
    other_tags = []
    
    # íƒœê·¸ ë¶„ë¥˜
    for tag in tags:
        if tag in CONFIG['TARGET_SHOTS']:
            found_shot = tag
        elif tag in CONFIG['TARGET_ANGLES']:
            found_angle = tag
        else:
            # ì´ë¯¸ íŠ¸ë¦¬ê±° í˜•ì‹ì´ë©´ ì¤‘ë³µ ë°©ì§€ ìœ„í•´ ì œì™¸, ì•„ë‹ˆë©´ ê¸°íƒ€ íƒœê·¸ë¡œ ë¶„ë¥˜
            if not (tag.startswith('<') and '_trg>' in tag):
                other_tags.append(tag)
    
    # ìƒ· ì •ë³´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if not found_shot:
        return text, False

    # íŠ¸ë¦¬ê±° ê²°ì •
    trigger = CONFIG['TRIGGER_MAP'].get(found_shot, '')
    
    # ìˆœì„œ ì¬ì¡°ë¦½: [íŠ¸ë¦¬ê±°] -> [ìƒ·] -> [ì•µê¸€] -> [ë‚˜ë¨¸ì§€]
    new_tags = [trigger, found_shot]
    if found_angle:
        new_tags.append(found_angle)
    new_tags.extend(other_tags)
    
    return ", ".join(new_tags), True

def main():
    print("ğŸš€ ìµœì¢… ë°ì´í„°ì…‹ í™•ì • íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"   - í† í° ì œí•œ: {CONFIG['TOKEN_LIMIT']} (Model: {CONFIG['TOKENIZER_MODEL']})")

    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    print("   - í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(CONFIG['TOKENIZER_MODEL'])
    except Exception as e:
        print(f"ğŸš¨ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # 2. íŒŒì¼ ì²˜ë¦¬ ë£¨í”„
    for file_info in CONFIG['FILES_TO_PROCESS']:
        input_path = file_info['input']
        output_path = file_info['output']
        
        print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {os.path.basename(input_path)}")
        
        processed_items = []
        stats = {
            "total_read": 0,
            "triggered": 0,
            "filtered_length": 0,
            "final_saved": 0
        }

        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip(): continue
                    stats['total_read'] += 1
                    
                    item = json.loads(line)
                    original_text = item.get('text', '')

                    # [ë‹¨ê³„ 1] í¬ë§·íŒ… (íŠ¸ë¦¬ê±° ì¶”ê°€ ë° ì •ë ¬)
                    formatted_text, is_triggered = reorder_and_add_trigger(original_text)
                    if is_triggered:
                        stats['triggered'] += 1
                    
                    # [ë‹¨ê³„ 2] í•„í„°ë§ (í† í° ê¸¸ì´ ê²€ì‚¬)
                    # ì£¼ì˜: í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸(formatted_text) ê¸°ì¤€ìœ¼ë¡œ ê¸¸ì´ë¥¼ ì¬ì•¼ í•¨!
                    token_ids = tokenizer(formatted_text, add_special_tokens=True).input_ids
                    if len(token_ids) > CONFIG['TOKEN_LIMIT']:
                        stats['filtered_length'] += 1
                        continue # ì €ì¥í•˜ì§€ ì•Šê³  ê±´ë„ˆëœ€

                    # [ë‹¨ê³„ 3] ì €ì¥ ëª©ë¡ì— ì¶”ê°€
                    item['text'] = formatted_text
                    processed_items.append(item)
                    stats['final_saved'] += 1

            # íŒŒì¼ ì €ì¥
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f_out:
                for item in processed_items:
                    json.dump(item, f_out, ensure_ascii=False)
                    f_out.write('\n')
            
            # ê²°ê³¼ ë¦¬í¬íŠ¸
            print(f"ğŸ“Š ê²°ê³¼ ìš”ì•½:")
            print(f"   - ì½ì€ ë°ì´í„°: {stats['total_read']}ê°œ")
            print(f"   - íŠ¸ë¦¬ê±° ë¶€ì°©: {stats['triggered']}ê°œ")
            print(f"   âŒ ê¸¸ì´ ì´ˆê³¼ ì œì™¸: {stats['filtered_length']}ê°œ")
            print(f"   âœ… ìµœì¢… ì €ì¥: {stats['final_saved']}ê°œ")
            print(f"   ğŸ“‚ ì €ì¥ ê²½ë¡œ: {output_path}")

        except FileNotFoundError:
            print(f"ğŸš¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
            continue

    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! í•™ìŠµ ê°€ëŠ¥í•œ ìµœì¢… ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()