import json
import re
import torch
import sys
import os
from transformers import pipeline, AutoTokenizer

# =============================================================================
# [Configuration] 
# =============================================================================
CONFIG = {
    # 1. ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ (ì£¼ì„ì„ í•´ì œ/ì„¤ì •í•˜ì—¬ ëª¨ë“œë¥¼ ë³€ê²½í•˜ì„¸ìš”)
    # "ALL":     í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ í•œê¸€ë¡œ ê°„ì£¼í•˜ê³  ë²ˆì—­í•©ë‹ˆë‹¤.
    # "PARTIAL": ì•ì— ìˆëŠ” ì˜ì–´(íŠ¸ë¦¬ê±° ë“±)ëŠ” ìœ ì§€í•˜ê³ , ë’¤ì— ë‚˜ì˜¤ëŠ” í•œê¸€ë§Œ ë²ˆì—­í•©ë‹ˆë‹¤.
    "TRANSLATION_MODE": "PARTIAL", 
    # "TRANSLATION_MODE": "ALL",

    # 2. ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ (ì…ë ¥ íŒŒì¼ -> ì¶œë ¥ íŒŒì¼)
    "FILES_TO_PROCESS": [
        {
            "input": "/home/aikusrv01/storyboard/PSY/preprocessed_for_translation.jsonl",
            "output": "/home/aikusrv01/storyboard/PSY/final_translated_dataset.jsonl"
        }
    ],

    # 3. ëª¨ë¸ ì„¤ì •
    "MODEL_NAME": "gyupro/Koalpaca-Translation-KR2EN",
    "BATCH_SIZE": 4, # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì ˆ (2~8)
}
# =============================================================================

def split_text_by_mode(text, mode):
    """
    ì„¤ì •ëœ ëª¨ë“œì— ë”°ë¼ í…ìŠ¤íŠ¸ë¥¼ (ë³´ì¡´í•  ì•ë¶€ë¶„, ë²ˆì—­í•  ë’·ë¶€ë¶„)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    """
    # [ëª¨ë“œ 1] ë¶€ë¶„ ë²ˆì—­ (ì˜ì–´ ìœ ì§€, í•œê¸€ë§Œ ë²ˆì—­)
    if mode == "PARTIAL":
        # ì²« ë²ˆì§¸ í•œê¸€(ê°€-í£)ì´ ë‚˜ì˜¤ëŠ” ìœ„ì¹˜ íƒìƒ‰
        match = re.search(r'[ê°€-í£]', text)
        if match:
            split_idx = match.start()
            
            # prefix: ì˜ì–´/íŠ¹ìˆ˜ë¬¸ì (ë²ˆì—­ X, ê·¸ëŒ€ë¡œ ìœ ì§€)
            prefix = text[:split_idx].strip()
            if prefix.endswith(','): prefix = prefix[:-1].strip()
            
            # suffix: í•œê¸€ í¬í•¨ ë’·ë¶€ë¶„ (ë²ˆì—­ O)
            suffix = text[split_idx:].strip()
            
            return prefix, suffix
        else:
            # í•œê¸€ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ prefixë¡œ ê°„ì£¼ (ë²ˆì—­ ì•ˆ í•¨)
            return text, ""

    # [ëª¨ë“œ 2] ì „ì²´ ë²ˆì—­ (í…ìŠ¤íŠ¸ í†µì§¸ë¡œ ë²ˆì—­)
    elif mode == "ALL":
        # prefix ì—†ìŒ, ì „ì²´ê°€ suffix(ë²ˆì—­ ëŒ€ìƒ)
        return "", text
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œì…ë‹ˆë‹¤: {mode}")

def build_prompt(korean_text):
    """Koalpaca í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return f"Korean: {korean_text}\nEnglish:"

def main():
    print(f"ğŸš€ ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ëª¨ë“œ: {CONFIG['TRANSLATION_MODE']})")
    
    # 1. ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒ)
    print(f"   - ëª¨ë¸ ë¡œë”© ì¤‘: {CONFIG['MODEL_NAME']}")
    try:
        device_index = 0 if torch.cuda.is_available() else -1
        tokenizer = AutoTokenizer.from_pretrained(CONFIG['MODEL_NAME'])
        
        # 8-bit ë¡œë”©ì´ í•„ìš”í•œ ê²½ìš° load_in_8bit=True ì‚¬ìš© (bitsandbytes í•„ìš”)
        generator = pipeline(
            "text-generation",
            model=CONFIG['MODEL_NAME'],
            tokenizer=tokenizer,
            device=device_index,
            torch_dtype=torch.float16
        )
        print("   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\n")
    except Exception as e:
        print(f"ğŸš¨ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # 2. íŒŒì¼ ëª©ë¡ ìˆœíšŒ
    for idx, file_info in enumerate(CONFIG['FILES_TO_PROCESS']):
        input_path = file_info['input']
        output_path = file_info['output']
        
        print(f"ğŸ“„ [íŒŒì¼ {idx+1}/{len(CONFIG['FILES_TO_PROCESS'])}] ì²˜ë¦¬ ì‹œì‘")
        print(f"   - ì…ë ¥: {os.path.basename(input_path)}")

        all_items = []
        texts_to_translate = [] # ì‹¤ì œë¡œ ëª¨ë¸ì— ë“¤ì–´ê°ˆ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        item_indices = []       # ë‚˜ì¤‘ì— ê²°ê³¼ë¥¼ ë§¤í•‘í•˜ê¸° ìœ„í•œ ì¸ë±ìŠ¤

        # 2-1. íŒŒì¼ ì½ê¸° ë° ë°ì´í„° ë¶„ë¦¬
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip(): continue
                    data = json.loads(line)
                    original_text = data.get('text', '')

                    # ëª¨ë“œì— ë”°ë¼ ë¶„ë¦¬ (ë³´ì¡´í•  ì•ë¶€ë¶„ / ë²ˆì—­í•  ë’·ë¶€ë¶„)
                    prefix, suffix = split_text_by_mode(original_text, CONFIG['TRANSLATION_MODE'])
                    
                    item = {
                        'file_name': data['file_name'],
                        'prefix': prefix,
                        'suffix': suffix,
                        'translated_suffix': ''
                    }
                    all_items.append(item)

                    # ë²ˆì—­í•  ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    if suffix:
                        texts_to_translate.append(suffix)
                        item_indices.append(len(all_items) - 1)
            
            print(f"   - ë°ì´í„° ë¡œë“œ: ì´ {len(all_items)}ê°œ (ë²ˆì—­ ëŒ€ìƒ: {len(texts_to_translate)}ê°œ)")

        except FileNotFoundError:
            print(f"   ğŸš¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤: {input_path}")
            continue

        # 2-2. ë°°ì¹˜ ë²ˆì—­ ì‹¤í–‰
        if texts_to_translate:
            print(f"   - ë²ˆì—­ ìˆ˜í–‰ ì¤‘ (Batch Size: {CONFIG['BATCH_SIZE']})...")
            
            prompts = [build_prompt(t) for t in texts_to_translate]
            total_prompts = len(prompts)
            
            for i in range(0, total_prompts, CONFIG['BATCH_SIZE']):
                batch_prompts = prompts[i : i + CONFIG['BATCH_SIZE']]
                
                try:
                    outputs = generator(
                        batch_prompts,
                        max_new_tokens=256,
                        do_sample=False,
                        num_beams=1,
                        return_full_text=False,
                        pad_token_id=tokenizer.eos_token_id
                    )
                    
                    for j, output in enumerate(outputs):
                        generated_text = output[0]['generated_text']
                        
                        # ê²°ê³¼ íŒŒì‹± ("English:" ë’·ë¶€ë¶„ ì¶”ì¶œ)
                        if "English:" in generated_text:
                            translation = generated_text.split("English:", 1)[1].strip()
                        else:
                            translation = generated_text.strip()
                        
                        # ì›ë³¸ ì•„ì´í…œì— ê²°ê³¼ ë§¤í•‘
                        global_idx = item_indices[i + j]
                        all_items[global_idx]['translated_suffix'] = translation

                    # ì§„í–‰ ìƒí™© ì¶œë ¥ (ê°„ëµí•˜ê²Œ)
                    current = min(i + CONFIG['BATCH_SIZE'], total_prompts)
                    if (current % 100 == 0) or (current == total_prompts):
                        print(f"     ... {current}/{total_prompts} ì™„ë£Œ")

                except Exception as e:
                    print(f"     ğŸš¨ ë°°ì¹˜ ë²ˆì—­ ì˜¤ë¥˜ ({i}~): {e}")

        # 2-3. ê²°ê³¼ ë³‘í•© ë° ì €ì¥
        print(f"   - ì €ì¥ ì¤‘: {os.path.basename(output_path)}")
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f_out:
                for item in all_items:
                    prefix = item['prefix']
                    trans = item['translated_suffix']
                    
                    # ìµœì¢… í…ìŠ¤íŠ¸ ì¡°ë¦½
                    if prefix and trans:
                        final_text = f"{prefix}, {trans}"
                    elif prefix:
                        final_text = prefix
                    elif trans:
                        final_text = trans
                    else:
                        final_text = ""
                    
                    new_entry = {
                        "file_name": item['file_name'],
                        "text": final_text
                    }
                    json.dump(new_entry, f_out, ensure_ascii=False)
                    f_out.write('\n')
            print("   âœ… ì €ì¥ ì™„ë£Œ!\n")
            
        except Exception as e:
            print(f"   ğŸš¨ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    print("ğŸ‰ ëª¨ë“  ë²ˆì—­ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()