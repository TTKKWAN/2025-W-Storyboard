import argparse
from pathlib import Path
import torch
import os

from safetensors.torch import load_file

from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from transformers import CLIPTokenizer, CLIPTextModel

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run Stable Diffusion with fine-tuned LoRA and Custom Tokens.")
    
    parser.add_argument("--base-model", default="/home/aikusrv01/storyboard/TK/stable_diffusion", help="ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--checkpoint", required=True, help="í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ í´ë” ê²½ë¡œ")
    parser.add_argument("--weight-name", default="pytorch_lora_weights.safetensors", help="LoRA íŒŒì¼ëª…")
    parser.add_argument("--trigger-word", default=None, help="í”„ë¡¬í”„íŠ¸ ë§¨ ì•ì— ì¶”ê°€í•  íŠ¸ë¦¬ê±° ì›Œë“œ")
    parser.add_argument("--prompt", default=None, required=True, help="í”„ë¡¬í”„íŠ¸")
    parser.add_argument("--negative-prompt", default="low quality, bad anatomy, worst quality, text, watermark, blurry, ugly", help="ë¶€ì • í”„ë¡¬í”„íŠ¸")
    parser.add_argument("--lora-scale", type=float, default=1.0, help="LoRA ê°•ë„")
    parser.add_argument("--guidance-scale", type=float, default=7.5)
    parser.add_argument("--num-inference-steps", type=int, default=30)
    parser.add_argument("--seed", type=int, default=None)
    parser.add_argument("--output", default="/home/aikusrv01/storyboard/TK/img_output/sample.png")
    parser.add_argument("--fuse-lora", action="store_true")
    parser.add_argument("--allow-downloads", action="store_false", dest="local_files_only")
    parser.set_defaults(local_files_only=True)
    return parser.parse_args()

def main() -> None:
    args = parse_args()
    checkpoint_path = Path(args.checkpoint)
    output_path = Path(args.output)
    # output_path.parent.TKdir(parents=True, exist_ok=True)

    print(f"ğŸ”¹ Loading Base Model from: {args.base_model}")

    # ========================================================================
    # [ìˆ˜ì • í•µì‹¬] ì²´í¬í¬ì¸íŠ¸ í´ë” êµ¬ì¡°ì— ë§ì¶° ìœ ì—°í•˜ê²Œ ë¡œë”©
    # ========================================================================
    tokenizer = None
    text_encoder = None
    
    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„ (í´ë” ë£¨íŠ¸ì— added_tokens.jsonì´ ìˆëŠ”ì§€ í™•ì¸)
    try:
        print(f"ğŸ” Looking for custom tokenizer in: {checkpoint_path}")
        # subfolder=Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ ë£¨íŠ¸ì—ì„œ ì§ì ‘ ì°¾ê²Œ í•¨
        tokenizer = CLIPTokenizer.from_pretrained(args.checkpoint, subfolder=None, local_files_only=args.local_files_only)
        print("âœ… Custom Tokenizer loaded successfully from checkpoint root!")
    except Exception as e:
        print(f"âš ï¸ Failed to load tokenizer from root: {e}")
        print("â„¹ï¸ Fallback: Using Base Model tokenizer.")
        tokenizer = CLIPTokenizer.from_pretrained(args.base_model, subfolder="tokenizer")
# ========================================================================
    # [ìˆ˜ì •ë¨] 2. í…ìŠ¤íŠ¸ ì¸ì½”ë” ê°•ì œ ë¡œë“œ (SafeTensors ì§ì ‘ ì£¼ì…)
    # ========================================================================
    from safetensors.torch import load_file # ìƒë‹¨ importì— ì¶”ê°€ í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œ í˜¸ì¶œí•´ë„ ë¨

    print(f"ğŸ” Loading Base Text Encoder first...")
    # 1) ì¼ë‹¨ ë² ì´ìŠ¤ ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    text_encoder = CLIPTextModel.from_pretrained(args.base_model, subfolder="text_encoder", torch_dtype=torch.float16)
    
    # 2) í† í¬ë‚˜ì´ì € í¬ê¸°ì— ë§ì¶° ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤ (ì´ê±¸ í•´ì•¼ ê°€ì¤‘ì¹˜ë¥¼ ë„£ì„ ìˆ˜ ìˆìŒ)
    text_encoder.resize_token_embeddings(len(tokenizer))
    
    # 3) ì²´í¬í¬ì¸íŠ¸ì— ìˆëŠ” model.safetensors (í•™ìŠµëœ í…ìŠ¤íŠ¸ ì¸ì½”ë” ê°€ì¤‘ì¹˜)ë¥¼ ë®ì–´ì”Œì›ë‹ˆë‹¤.
    custom_weights_path = checkpoint_path / "model.safetensors"
    
    if custom_weights_path.exists():
        print(f"â™»ï¸ Overwriting Text Encoder weights from: {custom_weights_path}")
        try:
            # safetensors íŒŒì¼ ë¡œë“œ
            state_dict = load_file(str(custom_weights_path))
            
            # ê°€ì¤‘ì¹˜ ê°•ì œ ì£¼ì… (strict=Falseë¡œ í•´ì„œ í˜•ì‹ì´ ì¡°ê¸ˆ ë‹¬ë¼ë„ ì¤‘ìš” ë¶€ë¶„ë§Œ ë¡œë“œ)
            missing, unexpected = text_encoder.load_state_dict(state_dict, strict=False)
            print("âœ… Custom Text Encoder weights loaded successfully!")
        except Exception as e:
            print(f"âš ï¸ Failed to load custom weights manually: {e}")
            print("â„¹ï¸ Running with random initialization for trigger word (Effect will be weak).")
    else:
        print("âš ï¸ model.safetensors not found in checkpoint. Trigger word might not work.")

    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipe = StableDiffusionPipeline.from_pretrained(
        args.base_model,
        tokenizer=tokenizer,
        text_encoder=text_encoder,
        torch_dtype=torch.float16,
        local_files_only=args.local_files_only,
        safety_checker=None,
        requires_safety_checker=False
    ).to("cuda")

    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)
    
    try:
        pipe.enable_xformers_memory_efficient_attention()
    except Exception:
        pass

    print(f"ğŸ”¹ Loading LoRA weights from: {args.checkpoint}")
    try:
        pipe.load_lora_weights(args.checkpoint, weight_name=args.weight_name)
        if args.fuse_lora:
            pipe.fuse_lora(lora_scale=args.lora_scale)
            print(f"âœ… LoRA Fused with scale: {args.lora_scale}")
    except Exception as e:
        print(f"âŒ Error loading LoRA: {e}")
        return

    generator = torch.Generator(device="cuda")
    if args.seed is not None:
        generator.manual_seed(args.seed)

    # í”„ë¡¬í”„íŠ¸ ì¡°í•©
    final_prompt = args.prompt
    if args.trigger_word:
        if final_prompt:
            final_prompt = f"{args.trigger_word}, {final_prompt}"
        else:
            final_prompt = args.trigger_word
            
    print(f"ğŸ“ Final Prompt: {final_prompt}")
    
    result = pipe(
        prompt=final_prompt,
        negative_prompt=args.negative_prompt,
        num_inference_steps=args.num_inference_steps,
        guidance_scale=args.guidance_scale,
        cross_attention_kwargs={"scale": args.lora_scale} if not args.fuse_lora else None,
        generator=generator,
    )
    
    image = result.images[0]
    image.save(output_path)
    print(f"ğŸ’¾ Saved Image to: {output_path}")

if __name__ == "__main__":
    main()